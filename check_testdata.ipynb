{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import torch\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # 파일로 저장하는 백엔드 사용\n",
    "\n",
    "# 파일 경로 설정\n",
    "test_path = './processed_test/'\n",
    "model_path = './new_model_save/time_series_cluster_models.pth'\n",
    "error_path = './result/reconstruction_errors.csv'\n",
    "output_path = './cluster_submission_2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 클래스 정의\n",
    "class TimeSeriesSensorAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=32, hidden_dim=256, window_size=1440, weight_decay=1e-4, sensor_weights=None):\n",
    "        super(TimeSeriesSensorAutoencoder, self).__init__()\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        if sensor_weights is None:\n",
    "            self.sensor_weights = {\n",
    "                'Q': 1.0,\n",
    "                'M': 0.5,\n",
    "                'P': 2.0\n",
    "            }\n",
    "        else:\n",
    "            self.sensor_weights = sensor_weights\n",
    "            \n",
    "        self.encoder_lstm = torch.nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.3,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        self.encoder_fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            torch.nn.BatchNorm1d(hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(hidden_dim, latent_dim),\n",
    "            torch.nn.BatchNorm1d(latent_dim),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.decoder_fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(latent_dim, hidden_dim),\n",
    "            torch.nn.BatchNorm1d(hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.decoder_lstm = torch.nn.LSTM(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.3,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        self.output_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            torch.nn.BatchNorm1d(hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        device = x.device\n",
    "        \n",
    "        lstm_out, _ = self.encoder_lstm(x)\n",
    "        last_hidden = lstm_out[:, -1, :]\n",
    "        z = self.encoder_fc(last_hidden)\n",
    "        \n",
    "        decoded = self.decoder_fc(z)\n",
    "        decoded = decoded.unsqueeze(1).repeat(1, x.size(1), 1)\n",
    "        decoded, _ = self.decoder_lstm(decoded)\n",
    "        reconstructed = self.output_layer(decoded[:, -1, :])\n",
    "        \n",
    "        return z, reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_cluster_error_distribution(errors_df, cluster_info, cluster_stats):\n",
    "    \"\"\"클러스터별 재구성 오류 분포 시각화 (TEST_C, TEST_D 분리)\"\"\"\n",
    "    # 각 클러스터의 오류 데이터 수집 (TEST_C, TEST_D 분리)\n",
    "    cluster_errors = {\n",
    "        'TEST_C': {cluster: [] for cluster in range(len(cluster_stats))},\n",
    "        'TEST_D': {cluster: [] for cluster in range(len(cluster_stats))}\n",
    "    }\n",
    "    \n",
    "    for file_id, errors, clusters in zip(\n",
    "        errors_df['ID'], \n",
    "        errors_df['error_list'].apply(ast.literal_eval), \n",
    "        [cluster_info[file_id] for file_id in errors_df['ID']]\n",
    "    ):\n",
    "        group = 'TEST_C' if file_id.startswith('TEST_C') else 'TEST_D'\n",
    "        for error, sensor_cluster in zip(errors, clusters):\n",
    "            if sensor_cluster != -1:\n",
    "                cluster_errors[group][sensor_cluster].append(error)\n",
    "    \n",
    "    # 각 그룹(TEST_C, TEST_D)에 대해 히스토그램 생성\n",
    "    for group in ['TEST_C', 'TEST_D']:\n",
    "        plt.figure(figsize=(20, 12))\n",
    "        for cluster, errors in cluster_errors[group].items():\n",
    "            if not errors:\n",
    "                continue\n",
    "            \n",
    "            plt.subplot(2, 3, cluster + 1)\n",
    "            plt.hist(errors, bins=50, edgecolor='black', alpha=0.7)\n",
    "            plt.title(f'{group} Cluster {cluster} Reconstruction Error Distribution')\n",
    "            plt.xlabel('Reconstruction Error')\n",
    "            plt.ylabel('Frequency')\n",
    "            \n",
    "            # 클러스터의 threshold 표시\n",
    "            threshold = cluster_stats[cluster]['threshold']\n",
    "            plt.axvline(x=threshold, color='r', linestyle='--', \n",
    "                        label=f'Original Threshold: {threshold:.4f}')\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'./{group}_cluster_error_distribution.png')\n",
    "        plt.close()\n",
    "\n",
    "    # 박스 플롯 (TEST_C, TEST_D 분리)\n",
    "    for group in ['TEST_C', 'TEST_D']:\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        error_data = [errors for errors in cluster_errors[group].values() if errors]\n",
    "        cluster_labels = [f'Cluster {i}' for i, errors in cluster_errors[group].items() if errors]\n",
    "        \n",
    "        plt.boxplot(error_data, tick_labels=cluster_labels)\n",
    "        plt.title(f'{group} Reconstruction Error Distribution by Cluster')\n",
    "        plt.ylabel('Reconstruction Error')\n",
    "        plt.xlabel('Cluster')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'./{group}_cluster_error_boxplot.png')\n",
    "        plt.close()\n",
    "\n",
    "    # 추가 통계 정보 출력 (TEST_C, TEST_D 분리)\n",
    "    for group in ['TEST_C', 'TEST_D']:\n",
    "        print(f\"\\n{group} 클러스터별 재구성 오류 통계:\")\n",
    "        for cluster, errors in cluster_errors[group].items():\n",
    "            if errors:\n",
    "                print(f\"Cluster {cluster}:\")\n",
    "                print(f\"  평균 오류: {np.mean(errors):.4f}\")\n",
    "                print(f\"  표준편차: {np.std(errors):.4f}\")\n",
    "                print(f\"  최대 오류: {np.max(errors):.4f}\")\n",
    "                print(f\"  최소 오류: {np.min(errors):.4f}\")\n",
    "\n",
    "# 나머지 기존 함수들 (이전 코드와 동일)\n",
    "def get_cluster_for_p_sensor(df, p_sensor, kmeans):\n",
    "    \"\"\"P센서의 원본 데이터 평균으로 클러스터 할당\"\"\"\n",
    "    p_mean = np.mean(df[p_sensor].values)  # 정규화하지 않은 원본 값의 평균\n",
    "    return kmeans.predict([[p_mean]])[0]\n",
    "\n",
    "def process_test_files(kmeans):\n",
    "    \"\"\"테스트 파일들의 P센서 클러스터 확인\"\"\"\n",
    "    file_list = sorted(glob(os.path.join(test_path, '*.csv')))\n",
    "    cluster_info = {}\n",
    "    cluster_counts = {i: 0 for i in range(kmeans.n_clusters)}  # 클러스터별 카운트\n",
    "    \n",
    "    for file_path in tqdm(file_list, desc=\"Processing files\"):\n",
    "        df = pd.read_csv(file_path)\n",
    "        file_name = os.path.basename(file_path).split('.')[0]\n",
    "        p_sensors = [col for col in df.columns if col.startswith('P') and not col.endswith('_flag')]\n",
    "        \n",
    "        clusters = []\n",
    "        for p_sensor in p_sensors:\n",
    "            cluster = get_cluster_for_p_sensor(df, p_sensor, kmeans)\n",
    "            clusters.append(cluster)\n",
    "            cluster_counts[cluster] += 1  # 클러스터별 카운트 증가\n",
    "            \n",
    "        cluster_info[file_name] = clusters\n",
    "    \n",
    "    # 클러스터 분포 출력\n",
    "    print(\"\\nCluster Distribution:\")\n",
    "    for cluster, count in cluster_counts.items():\n",
    "        print(f\"Cluster {cluster}: {count} sensors\")\n",
    "    \n",
    "    return cluster_info\n",
    "\n",
    "def recommend_cluster_search_ranges(cluster_stats):\n",
    "    \"\"\"사용자가 직접 클러스터별 threshold scale 탐색 범위 설정\"\"\"\n",
    "    search_ranges = {}\n",
    "    \n",
    "    # 클러스터 threshold 정보 출력\n",
    "    print(\"\\n클러스터별 원본 Threshold 정보:\")\n",
    "    for cluster, stats in cluster_stats.items():\n",
    "        original_threshold = stats['threshold']\n",
    "        print(f\"Cluster {cluster}: Original Threshold = {original_threshold:.4f}\")\n",
    "    \n",
    "    # 사용자 입력 안내\n",
    "    print(\"\\n각 클러스터의 threshold scale 탐색 범위를 설정해주세요.\")\n",
    "    print(\"형식: 클러스터 번호 최소값 최대값 (예: 0 5 7)\")\n",
    "    print(\"모든 입력이 완료되면 'done'을 입력하세요.\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"입력: \").strip()\n",
    "        \n",
    "        if user_input.lower() == 'done':\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            cluster, left, right = map(float, user_input.split())\n",
    "            cluster = int(cluster)\n",
    "            search_ranges[cluster] = (left, right)\n",
    "        except ValueError:\n",
    "            print(\"잘못된 입력입니다. 다시 입력해주세요.\")\n",
    "    \n",
    "    # 입력되지 않은 클러스터는 기본값 사용\n",
    "    for cluster in range(len(cluster_stats)):\n",
    "        if cluster not in search_ranges:\n",
    "            search_ranges[cluster] = (13, 14)\n",
    "    \n",
    "    print(\"\\n최종 Threshold Scale 탐색 범위:\")\n",
    "    for cluster, (left, right) in search_ranges.items():\n",
    "        print(f\"Cluster {cluster}: Scale 탐색 범위 = ({left}, {right})\")\n",
    "    \n",
    "    return search_ranges\n",
    "\n",
    "def get_cluster_for_p_sensor(df, p_sensor, kmeans):\n",
    "    \"\"\"P센서의 원본 데이터 평균으로 클러스터 할당\"\"\"\n",
    "    p_mean = np.mean(df[p_sensor].values)  # 정규화하지 않은 원본 값의 평균\n",
    "    return kmeans.predict([[p_mean]])[0]\n",
    "\n",
    "def process_test_files(kmeans):\n",
    "    \"\"\"테스트 파일들의 P센서 클러스터 확인\"\"\"\n",
    "    file_list = sorted(glob(os.path.join(test_path, '*.csv')))\n",
    "    cluster_info = {}\n",
    "    cluster_counts = {i: 0 for i in range(kmeans.n_clusters)}  # 클러스터별 카운트\n",
    "    \n",
    "    for file_path in tqdm(file_list, desc=\"Processing files\"):\n",
    "        df = pd.read_csv(file_path)\n",
    "        file_name = os.path.basename(file_path).split('.')[0]\n",
    "        p_sensors = [col for col in df.columns if col.startswith('P') and not col.endswith('_flag')]\n",
    "        \n",
    "        clusters = []\n",
    "        for p_sensor in p_sensors:\n",
    "            cluster = get_cluster_for_p_sensor(df, p_sensor, kmeans)\n",
    "            clusters.append(cluster)\n",
    "            cluster_counts[cluster] += 1  # 클러스터별 카운트 증가\n",
    "            \n",
    "        cluster_info[file_name] = clusters\n",
    "    \n",
    "    # 클러스터 분포 출력\n",
    "    print(\"\\nCluster Distribution:\")\n",
    "    for cluster, count in cluster_counts.items():\n",
    "        print(f\"Cluster {cluster}: {count} sensors\")\n",
    "    \n",
    "    return cluster_info\n",
    "\n",
    "def count_valid_samples(submission_df):\n",
    "    \"\"\"유효 샘플(적어도 하나의 이상이 감지된 파일) 수를 계산\"\"\"\n",
    "    valid_count = 0\n",
    "    for flags in submission_df['flag_list']:\n",
    "        flags_list = ast.literal_eval(flags)\n",
    "        if sum(flags_list) > 0:  # 하나라도 1이 있으면 유효 샘플\n",
    "            valid_count += 1\n",
    "    return valid_count\n",
    "\n",
    "def create_submission_with_custom_threshold(errors_df, cluster_info, cluster_stats, custom_threshold):\n",
    "    \"\"\"특정 threshold로 결과 생성\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for _, row in errors_df.iterrows():\n",
    "        file_id = row['ID']\n",
    "        errors = ast.literal_eval(row['error_list'])\n",
    "        clusters = cluster_info[file_id]\n",
    "        \n",
    "        anomaly_flags = []\n",
    "        for error, cluster in zip(errors, clusters):\n",
    "            # 클러스터가 -1인 경우 이상 감지를 하지 않음\n",
    "            if cluster == -1:\n",
    "                anomaly_flags.append(0)\n",
    "            else:\n",
    "                is_anomaly = 1 if error > custom_threshold else 0\n",
    "                anomaly_flags.append(is_anomaly)\n",
    "        \n",
    "        results.append({\n",
    "            'ID': file_id,\n",
    "            'flag_list': str(anomaly_flags)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def find_optimal_threshold_scale_by_cluster(\n",
    "    errors_df, \n",
    "    cluster_info, \n",
    "    cluster_stats, \n",
    "    custom_thresholds\n",
    "):\n",
    "    \"\"\"클러스터별로 설정된 threshold 적용\"\"\"\n",
    "    cluster_scales = {}\n",
    "    \n",
    "    # TEST_C와 TEST_D 그룹 분리\n",
    "    test_c_files = errors_df[errors_df['ID'].str.startswith('TEST_C')]['ID'].tolist()\n",
    "    test_d_files = errors_df[errors_df['ID'].str.startswith('TEST_D')]['ID'].tolist()\n",
    "    \n",
    "    # 그룹별 처리\n",
    "    for group, files in [('TEST_C', test_c_files), ('TEST_D', test_d_files)]:\n",
    "        # 현재 그룹의 데이터만 필터링\n",
    "        group_errors_df = errors_df[errors_df['ID'].isin(files)]\n",
    "        group_cluster_info = {k: v for k, v in cluster_info.items() if k in files}\n",
    "        \n",
    "        # 유효한 총 클러스터 목록 추출\n",
    "        used_clusters = set(cluster for clusters in group_cluster_info.values() for cluster in clusters if cluster != -1)\n",
    "        \n",
    "        for cluster in used_clusters:\n",
    "            # 현재 클러스터의 데이터만 포함하는 cluster_info 생성\n",
    "            cluster_specific_info = {\n",
    "                file_id: [c if c == cluster else -1 for c in clusters]\n",
    "                for file_id, clusters in group_cluster_info.items()\n",
    "            }\n",
    "            \n",
    "            # 사용자 정의 threshold 사용\n",
    "            custom_threshold = custom_thresholds[group].get(cluster, cluster_stats[cluster]['threshold'])\n",
    "            \n",
    "            # 서브밋 생성\n",
    "            submission_df = create_submission_with_custom_threshold(\n",
    "                group_errors_df, \n",
    "                cluster_specific_info, \n",
    "                cluster_stats, \n",
    "                custom_threshold\n",
    "            )\n",
    "            \n",
    "            # 유효 샘플 수 계산\n",
    "            valid_count = count_valid_samples(submission_df)\n",
    "            \n",
    "            # 기존 코드와 호환되는 형식으로 저장\n",
    "            cluster_scales[cluster] = {\n",
    "                'scale': custom_threshold / cluster_stats[cluster]['threshold'],  # scale 계산\n",
    "                'valid_count': valid_count,\n",
    "                'threshold': custom_threshold\n",
    "            }\n",
    "    \n",
    "    return cluster_scales\n",
    "\n",
    "def create_final_submission_by_cluster(\n",
    "    errors_df, \n",
    "    cluster_info, \n",
    "    cluster_stats, \n",
    "    cluster_scales\n",
    "):\n",
    "    results = []\n",
    "    \n",
    "    for _, row in errors_df.iterrows():\n",
    "        file_id = row['ID']\n",
    "        errors = ast.literal_eval(row['error_list'])\n",
    "        clusters = cluster_info[file_id]\n",
    "        \n",
    "        anomaly_flags = []\n",
    "        for error, cluster in zip(errors, clusters):\n",
    "            # 클러스터가 -1인 경우 이상 감지를 하지 않음\n",
    "            if cluster == -1:\n",
    "                anomaly_flags.append(0)\n",
    "            else:\n",
    "                # 해당 클러스터의 threshold 가져오기\n",
    "                threshold = cluster_scales[cluster]['threshold']\n",
    "                is_anomaly = 1 if error > threshold else 0\n",
    "                anomaly_flags.append(is_anomaly)\n",
    "        \n",
    "        results.append({\n",
    "            'ID': file_id,\n",
    "            'flag_list': str(anomaly_flags)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def recommend_cluster_search_ranges(cluster_stats):\n",
    "    \"\"\"TEST_C와 TEST_D별로 클러스터별 threshold를 하이퍼파라미터로 설정\"\"\"\n",
    "    thresholds = {\n",
    "        'TEST_C': {\n",
    "            3: 50,    # TEST_C Cluster 3의 threshold\n",
    "            5: 50    # TEST_C Cluster 5의 threshold\n",
    "        },\n",
    "        'TEST_D': {\n",
    "            2: 9.493,      # TEST_D Cluster 2의 threshold\n",
    "            4: 4.54,      # TEST_D Cluster 4의 threshold\n",
    "            5: 2.714       # TEST_D Cluster 5의 threshold\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 클러스터 threshold 정보 출력\n",
    "    print(\"\\n클러스터별 Threshold 설정:\")\n",
    "    for group in ['TEST_C', 'TEST_D']:\n",
    "        print(f\"\\n{group} 그룹:\")\n",
    "        for cluster, threshold in thresholds[group].items():\n",
    "            original_threshold = cluster_stats[cluster]['threshold']\n",
    "            print(f\"Cluster {cluster}: 원본 Threshold = {original_threshold:.4f}, 설정된 Threshold = {threshold:.4f}\")\n",
    "    \n",
    "    return thresholds\n",
    "\n",
    "def print_detailed_cluster_info(cluster_scales, cluster_stats):\n",
    "    \"\"\"클러스터별 상세 정보 출력\"\"\"\n",
    "    print(\"\\n상세 클러스터 정보:\")\n",
    "    for cluster, info in cluster_scales.items():\n",
    "        original_threshold = cluster_stats[cluster]['threshold']\n",
    "        adjusted_threshold = original_threshold * info['scale']\n",
    "        print(f\"Cluster {cluster}:\")\n",
    "        print(f\"  원본 Threshold: {original_threshold:.4f}\")\n",
    "        print(f\"  Scale: {info['scale']:.4f}\")\n",
    "        print(f\"  조정된 Threshold: {adjusted_threshold:.4f}\")\n",
    "        print(f\"  유효 샘플 수: {info['valid_count']}\")\n",
    "\n",
    "def calculate_performance_metrics(final_submission):\n",
    "    \"\"\"이상 탐지 성능에 대한 상세 메트릭 계산\"\"\"\n",
    "    total_anomalies = 0\n",
    "    total_sensors = 0\n",
    "    test_c_anomalies = 0\n",
    "    test_d_anomalies = 0\n",
    "\n",
    "    for row in final_submission.itertuples():\n",
    "        file_id = row.ID\n",
    "        flags_list = ast.literal_eval(row.flag_list)\n",
    "        \n",
    "        file_anomalies = sum(flags_list)\n",
    "        total_anomalies += file_anomalies\n",
    "        total_sensors += len(flags_list)\n",
    "        \n",
    "        if file_id.startswith('TEST_C'):\n",
    "            test_c_anomalies += file_anomalies\n",
    "        else:\n",
    "            test_d_anomalies += file_anomalies\n",
    "\n",
    "    print(\"\\n성능 평가:\")\n",
    "    print(f\"전체 이상 감지 비율: {total_anomalies / total_sensors * 100:.2f}%\")\n",
    "    print(f\"전체 센서 중 이상 센서 비율: {total_anomalies} / {total_sensors}\")\n",
    "    print(f\"TEST_C 이상 감지 비율: {test_c_anomalies / sum(1 for row in final_submission.itertuples() if row.ID.startswith('TEST_C')) * 100:.2f}%\")\n",
    "    print(f\"TEST_D 이상 감지 비율: {test_d_anomalies / sum(1 for row in final_submission.itertuples() if row.ID.startswith('TEST_D')) * 100:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'total_anomaly_ratio': total_anomalies / total_sensors * 100,\n",
    "        'total_anomalies': total_anomalies,\n",
    "        'total_sensors': total_sensors,\n",
    "        'test_c_anomaly_ratio': test_c_anomalies / sum(1 for row in final_submission.itertuples() if row.ID.startswith('TEST_C')) * 100,\n",
    "        'test_d_anomaly_ratio': test_d_anomalies / sum(1 for row in final_submission.itertuples() if row.ID.startswith('TEST_D')) * 100\n",
    "    }\n",
    "\n",
    "def analyze_cluster_anomalies(errors_df, cluster_info, cluster_stats, cluster_scales):\n",
    "    \"\"\"클러스터별 이상 센서 상세 분석\"\"\"\n",
    "    cluster_anomaly_info = {}\n",
    "    \n",
    "    for cluster in range(len(cluster_stats)):\n",
    "        # 해당 클러스터의 모든 센서 추출\n",
    "        cluster_sensors = [\n",
    "            (file_id, errors, clusters) \n",
    "            for file_id, errors, clusters in zip(\n",
    "                errors_df['ID'], \n",
    "                errors_df['error_list'].apply(ast.literal_eval), \n",
    "                [cluster_info[file_id] for file_id in errors_df['ID']]\n",
    "            )\n",
    "            if cluster in clusters\n",
    "        ]\n",
    "        \n",
    "        # 클러스터별 이상 감지 분석\n",
    "        total_sensors = len(cluster_sensors)\n",
    "        \n",
    "        # 전체 센서 수가 0이면 해당 클러스터 제외\n",
    "        if total_sensors == 0:\n",
    "            continue\n",
    "        \n",
    "        anomaly_sensors = 0\n",
    "        max_error = 0\n",
    "        min_error = float('inf')\n",
    "        errors_list = []\n",
    "        \n",
    "        for file_id, errors, clusters in cluster_sensors:\n",
    "            for error, sensor_cluster in zip(errors, clusters):\n",
    "                if sensor_cluster == cluster:\n",
    "                    errors_list.append(error)\n",
    "                    \n",
    "                    if error > cluster_stats[cluster]['threshold']:\n",
    "                        anomaly_sensors += 1\n",
    "                    \n",
    "                    max_error = max(max_error, error)\n",
    "                    min_error = min(min_error, error)\n",
    "        \n",
    "        cluster_anomaly_info[cluster] = {\n",
    "            'total_sensors': total_sensors,\n",
    "            'anomaly_sensors': anomaly_sensors,\n",
    "            'anomaly_ratio': anomaly_sensors / total_sensors * 100,\n",
    "            'max_error': max_error,\n",
    "            'min_error': min_error,\n",
    "            'mean_error': np.mean(errors_list),\n",
    "            'scale': cluster_scales[cluster]['scale'] if cluster in cluster_scales else None\n",
    "        }\n",
    "    \n",
    "    # 결과 출력\n",
    "    print(\"\\n클러스터별 상세 이상 감지 분석:\")\n",
    "    for cluster, info in cluster_anomaly_info.items():\n",
    "        original_threshold = cluster_stats[cluster]['threshold']\n",
    "        adjusted_threshold = original_threshold * info['scale'] if info['scale'] is not None else None\n",
    "        \n",
    "        print(f\"Cluster {cluster}:\")\n",
    "        print(f\"  전체 센서 수: {info['total_sensors']}\")\n",
    "        print(f\"  이상 센서 수: {info['anomaly_sensors']}\")\n",
    "        print(f\"  이상 센서 비율: {info['anomaly_ratio']:.2f}%\")\n",
    "        print(f\"  원본 Threshold: {original_threshold:.4f}\")\n",
    "        print(f\"  Scale: {info['scale']:.4f}\" if info['scale'] is not None else \"  Scale: N/A\")\n",
    "        print(f\"  조정된 Threshold: {adjusted_threshold:.4f}\" if adjusted_threshold is not None else \"  조정된 Threshold: N/A\")\n",
    "        print(f\"  평균 에러: {info['mean_error']:.4f}\")\n",
    "        print(f\"  최대 에러: {info['max_error']:.4f}\")\n",
    "        print(f\"  최소 에러: {info['min_error']:.4f}\")\n",
    "    \n",
    "    return cluster_anomaly_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(model_path)\n",
    "kmeans = checkpoint['kmeans']\n",
    "cluster_stats = checkpoint['cluster_stats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_df = pd.read_csv(error_path)\n",
    "cluster_info = process_test_files(kmeans)\n",
    "print(\"\\nFinding optimal threshold scale...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클러스터별 threshold 설정\n",
    "custom_thresholds = recommend_cluster_search_ranges(cluster_stats)\n",
    "\n",
    "# 메인 스크립트에서 함수 호출 수정\n",
    "cluster_scales = find_optimal_threshold_scale_by_cluster(\n",
    "    errors_df, \n",
    "    cluster_info, \n",
    "    cluster_stats, \n",
    "    custom_thresholds\n",
    ")\n",
    "\n",
    "# 상세 클러스터 정보 출력\n",
    "print_detailed_cluster_info(cluster_scales, cluster_stats)\n",
    "\n",
    "# 클러스터 이상 감지 분석 (cluster_scales 전달)\n",
    "cluster_anomaly_info = analyze_cluster_anomalies(\n",
    "    errors_df, \n",
    "    cluster_info, \n",
    "    cluster_stats, \n",
    "    cluster_scales  # 이 부분이 중요\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_submission = create_final_submission_by_cluster(\n",
    "    errors_df, \n",
    "    cluster_info, \n",
    "    cluster_stats, \n",
    "    cluster_scales\n",
    ")\n",
    "final_submission.to_csv(output_path, index=False)\n",
    "print(f\"\\nResults saved to {output_path}\")\n",
    "\n",
    "# 성능 메트릭 계산\n",
    "performance_metrics = calculate_performance_metrics(final_submission)\n",
    "\n",
    "# 클러스터별 재구성 오류 분포 시각화\n",
    "visualize_cluster_error_distribution(errors_df, cluster_info, cluster_stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yangpa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# 데이터 경로 설정\n",
    "train_path = './train/'\n",
    "output_path = './processed_7_480/'  # 데이터를 저장할 경로\n",
    "\n",
    "# 데이터 저장 폴더 생성\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "### 1단계: 데이터 로드 및 초기 설정 ###\n",
    "def load_data(path):\n",
    "    # 데이터 로드\n",
    "    file_list = glob(os.path.join(path, '*.csv'))\n",
    "    dataframes = [pd.read_csv(file) for file in file_list]\n",
    "    return dataframes\n",
    "\n",
    "### 2단계: 학습 데이터 준비 및 전처리 ###\n",
    "def split_to_weekly(data, window_size=7*24*60, step=480, exclusion_period=30):\n",
    "    segments = []\n",
    "    for start in range(0, len(data) - window_size + 1, step):\n",
    "        segment = data[start:start + window_size]\n",
    "        next_10_min = data[start + window_size: start + window_size + exclusion_period]\n",
    "\n",
    "        # anomaly가 포함되었거나 직후 10분 내에 anomaly가 있는 경우 제외\n",
    "        if segment['anomaly'].sum() == 0 and next_10_min['anomaly'].sum() == 0:\n",
    "            segments.append(segment)\n",
    "\n",
    "    return segments\n",
    "\n",
    "# 학습 데이터 로드 및 일주일 단위로 분할\n",
    "train_dataframes = load_data(train_path)\n",
    "train_segments = []\n",
    "\n",
    "# 모든 데이터프레임을 분할하여 저장\n",
    "for idx, df in enumerate(train_dataframes):\n",
    "    # 파일 이름에 따라 다르게 저장\n",
    "    file_prefix = f\"TRAIN_{chr(65 + idx)}\"\n",
    "    segments = split_to_weekly(df)\n",
    "    for seg_idx, segment in enumerate(segments):\n",
    "        file_name = f\"{file_prefix}_week_{seg_idx + 1}.csv\"\n",
    "        segment.to_csv(os.path.join(output_path, file_name), index=False)\n",
    "\n",
    "print(\"데이터 전처리 완료. 정상 데이터만 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 전처리된 데이터 경로와 새롭게 가공된 데이터를 저장할 경로 설정\n",
    "input_path = './processed_7_480/'\n",
    "output_path = './aggregated_7_480/'\n",
    "\n",
    "# 데이터 저장 폴더 생성\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "### 1단계: 데이터 로드 및 초기 설정 ###\n",
    "def load_data(path):\n",
    "    # 데이터 로드\n",
    "    file_list = glob(os.path.join(path, '*.csv'))\n",
    "    dataframes = [pd.read_csv(file) for file in file_list]\n",
    "    return dataframes, file_list\n",
    "\n",
    "### 2단계: M 센서 합치기 ###\n",
    "def aggregate_m_columns(data):\n",
    "    # M으로 시작하는 모든 센서 칼럼 선택\n",
    "    m_columns = [col for col in data.columns if col.startswith('M')]\n",
    "    \n",
    "    # M 센서의 값 합산\n",
    "    data['M_sum'] = data[m_columns].sum(axis=1)\n",
    "\n",
    "    # 기존 M 센서 칼럼 삭제\n",
    "    data = data.drop(columns=m_columns)\n",
    "\n",
    "    return data\n",
    "\n",
    "# 기존 전처리된 파일 로드\n",
    "processed_dataframes, file_list = load_data(input_path)\n",
    "\n",
    "# M 센서 합치기 및 새로운 파일로 저장\n",
    "for df, file_path in zip(processed_dataframes, file_list):\n",
    "    # 데이터 재가공\n",
    "    aggregated_df = aggregate_m_columns(df)\n",
    "    \n",
    "    # 파일 이름 설정 및 저장\n",
    "    file_name = os.path.basename(file_path)  # 기존 파일 이름을 유지하되 경로를 변경하여 저장\n",
    "    aggregated_df.to_csv(os.path.join(output_path, file_name), index=False)\n",
    "\n",
    "print(\"M 센서 합치기 작업 완료. 새로운 데이터가 'aggregated_data' 폴더에 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yangpa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "train_path = './aggregated_7_480/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_p_sensor_data(path):\n",
    "    file_list = glob(os.path.join(path, '*.csv'))\n",
    "    p_series = []\n",
    "    sensor_ids = []\n",
    "\n",
    "    for file in tqdm(file_list, desc=\"Processing files\", leave=False):\n",
    "        df = pd.read_csv(file)\n",
    "        p_sensors = [col for col in df.columns if col.startswith('P') and not col.endswith('_flag')]\n",
    "\n",
    "        for p_sensor in p_sensors:\n",
    "            # Extract various time series features\n",
    "            p_data = df[p_sensor].values\n",
    "            features = [\n",
    "                np.mean(p_data),       # Mean\n",
    "                np.median(p_data),     # Median\n",
    "                np.std(p_data),        # Standard Deviation\n",
    "                skew(p_data),          # Skewness\n",
    "                kurtosis(p_data),      # Kurtosis\n",
    "                np.percentile(p_data, 25),  # 1st Quartile\n",
    "                np.percentile(p_data, 75),  # 3rd Quartile\n",
    "                np.max(p_data) - np.min(p_data)  # Range\n",
    "            ]\n",
    "            \n",
    "            p_series.append(features)\n",
    "            sensor_id = f\"{'TRAIN_A' if 'TRAIN_A' in file else 'TRAIN_B'}_{p_sensor}\"\n",
    "            sensor_ids.append(sensor_id)\n",
    "\n",
    "    # Scaling\n",
    "    scaler = StandardScaler()\n",
    "    p_values_scaled = scaler.fit_transform(p_series)\n",
    "\n",
    "    return p_values_scaled, sensor_ids\n",
    "\n",
    "def calculate_silhouette_scores(p_values, max_clusters=15):\n",
    "    # Clustering methods\n",
    "    clustering_methods = {\n",
    "        'K-means': KMeans(n_init=10, random_state=42),\n",
    "        'Hierarchical': AgglomerativeClustering(),\n",
    "        'Gaussian Mixture': GaussianMixture(random_state=42)\n",
    "    }\n",
    "\n",
    "    # Dictionary to store silhouette scores\n",
    "    silhouette_scores = {method: [] for method in clustering_methods.keys()}\n",
    "\n",
    "    for method_name, clusterer in clustering_methods.items():\n",
    "        for n_clusters in range(2, max_clusters + 1):\n",
    "            try:\n",
    "                # Perform clustering based on method\n",
    "                if method_name == 'K-means':\n",
    "                    clusterer.n_clusters = n_clusters\n",
    "                    cluster_labels = clusterer.fit_predict(p_values)\n",
    "                elif method_name == 'Hierarchical':\n",
    "                    clusterer.n_clusters = n_clusters\n",
    "                    cluster_labels = clusterer.fit_predict(p_values)\n",
    "                else:  # Gaussian Mixture\n",
    "                    clusterer.n_components = n_clusters\n",
    "                    cluster_labels = clusterer.fit_predict(p_values)\n",
    "\n",
    "                # Calculate silhouette score only if there's more than one cluster\n",
    "                if len(np.unique(cluster_labels)) > 1:\n",
    "                    score = silhouette_score(p_values, cluster_labels)\n",
    "                    silhouette_scores[method_name].append(score)\n",
    "                else:\n",
    "                    silhouette_scores[method_name].append(0)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error with {method_name}, {n_clusters} clusters: {e}\")\n",
    "                silhouette_scores[method_name].append(0)\n",
    "\n",
    "    return silhouette_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare P sensor data\n",
    "p_values, sensor_ids = prepare_p_sensor_data(train_path)\n",
    "\n",
    "# Calculate silhouette scores\n",
    "silhouette_scores = calculate_silhouette_scores(p_values)\n",
    "\n",
    "# Plot silhouette scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "for method, scores in silhouette_scores.items():\n",
    "    plt.plot(range(2, len(scores) + 2), scores, marker='o', label=method)\n",
    "\n",
    "plt.title('Silhouette Score by Number of Clusters and Method')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best clustering method and number of clusters\n",
    "best_method = max(\n",
    "    silhouette_scores.items(), \n",
    "    key=lambda x: max(x[1]) if len(x[1]) > 0 else -1\n",
    ")[0]\n",
    "\n",
    "best_n_clusters = silhouette_scores[best_method].index(max(silhouette_scores[best_method])) + 2\n",
    "print(f\"\\nBest Clustering Method: {best_method}\")\n",
    "print(f\"Best Number of Clusters: {best_n_clusters}\")\n",
    "\n",
    "# Sensor ID and cluster mapping (if K-means is best)\n",
    "if best_method == 'K-means':\n",
    "    kmeans = KMeans(n_clusters=best_n_clusters, n_init=10, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(p_values)\n",
    "    \n",
    "    print(\"\\nSensor Cluster Mapping:\")\n",
    "    for sensor_id, cluster in zip(sensor_ids, cluster_labels):\n",
    "        print(f\"{sensor_id}: Cluster {cluster}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distribution visualization\n",
    "plt.figure(figsize=(20, 10))\n",
    "feature_names = [\n",
    "    'Mean', 'Median', 'Std Deviation', \n",
    "    'Skewness', 'Kurtosis', '1st Quartile', \n",
    "    '3rd Quartile', 'Range'\n",
    "]\n",
    "\n",
    "# Perform K-means clustering\n",
    "kmeans = KMeans(n_clusters=best_n_clusters, n_init=10, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(p_values)\n",
    "\n",
    "# Boxplot for each feature\n",
    "for i in range(len(feature_names)):\n",
    "    plt.subplot(2, 4, i+1)\n",
    "    for cluster in range(best_n_clusters):\n",
    "        cluster_data = p_values[cluster_labels == cluster, i]\n",
    "        plt.boxplot(cluster_data, positions=[cluster], widths=0.6)\n",
    "    \n",
    "    plt.title(f'{feature_names[i]} Distribution')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Normalized Value')\n",
    "    plt.xticks(range(best_n_clusters))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# PCA visualization\n",
    "pca = PCA(n_components=2)\n",
    "p_values_pca = pca.fit_transform(p_values)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    p_values_pca[:, 0], \n",
    "    p_values_pca[:, 1], \n",
    "    c=cluster_labels, \n",
    "    cmap='viridis'\n",
    ")\n",
    "plt.colorbar(scatter)\n",
    "plt.title('P Sensor Characteristics in PCA Space')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print sensors in each cluster\n",
    "print(\"\\nSensors in Each Cluster:\")\n",
    "for cluster in range(best_n_clusters):\n",
    "    cluster_sensors = [sensor_ids[i] for i in range(len(sensor_ids)) if cluster_labels[i] == cluster]\n",
    "    print(f\"Cluster {cluster}: {cluster_sensors}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yangpa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

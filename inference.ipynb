{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import torch\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "# 파일 경로 설정\n",
    "test_path = './processed_test/'\n",
    "model_path = './new_model_save/time_series_cluster_models.pth'\n",
    "error_path = './result/reconstruction_errors.csv'\n",
    "output_path = './cluster_submission_2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 클래스 정의\n",
    "class TimeSeriesSensorAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=32, hidden_dim=256):\n",
    "        super(TimeSeriesSensorAutoencoder, self).__init__()\n",
    "        self.encoder_lstm = torch.nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.3,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.encoder_fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim * 2, latent_dim),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.decoder_fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(latent_dim, hidden_dim),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.decoder_lstm = torch.nn.LSTM(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.3,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.output_layer = torch.nn.Linear(hidden_dim * 2, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.encoder_lstm(x)\n",
    "        z = self.encoder_fc(lstm_out[:, -1, :])\n",
    "        decoded = self.decoder_fc(z).unsqueeze(1).repeat(1, x.size(1), 1)\n",
    "        decoded, _ = self.decoder_lstm(decoded)\n",
    "        return z, self.output_layer(decoded[:, -1, :])\n",
    "\n",
    "# 데이터 처리 함수\n",
    "def get_cluster_for_p_sensor(df, p_sensor, kmeans):\n",
    "    p_mean = np.mean(df[p_sensor].values)\n",
    "    return kmeans.predict([[p_mean]])[0]\n",
    "\n",
    "def process_test_files(kmeans):\n",
    "    file_list = sorted(glob(os.path.join(test_path, '*.csv')))\n",
    "    cluster_info = {}\n",
    "    for file_path in tqdm(file_list, desc=\"Processing files\"):\n",
    "        df = pd.read_csv(file_path)\n",
    "        file_name = os.path.basename(file_path).split('.')[0]\n",
    "        p_sensors = [col for col in df.columns if col.startswith('P') and not col.endswith('_flag')]\n",
    "        clusters = [get_cluster_for_p_sensor(df, p_sensor, kmeans) for p_sensor in p_sensors]\n",
    "        cluster_info[file_name] = clusters\n",
    "    return cluster_info\n",
    "\n",
    "def create_submission_with_threshold(errors_df, cluster_info, thresholds):\n",
    "    results = []\n",
    "    for _, row in errors_df.iterrows():\n",
    "        file_id = row['ID']\n",
    "        errors = ast.literal_eval(row['error_list'])\n",
    "        clusters = cluster_info[file_id]\n",
    "\n",
    "        # 파일별 활성화된 클러스터 확인\n",
    "        active_clusters = thresholds.get(file_id[:6], {})\n",
    "\n",
    "        anomaly_flags = [\n",
    "            1 if cluster in active_clusters and error > active_clusters.get(cluster, float('inf')) else 0\n",
    "            for error, cluster in zip(errors, clusters)\n",
    "        ]\n",
    "        results.append({'ID': file_id, 'flag_list': str(anomaly_flags)})\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "checkpoint = torch.load(model_path)\n",
    "kmeans = checkpoint['kmeans']\n",
    "cluster_stats = checkpoint['cluster_stats']\n",
    "errors_df = pd.read_csv(error_path)\n",
    "\n",
    "cluster_info = process_test_files(kmeans)\n",
    "\n",
    "# 사용자 정의 Threshold 설정\n",
    "thresholds = {\n",
    "    'TEST_C': {3: 7.25, 5: 10},  # TEST_C 클러스터별 Threshold\n",
    "    'TEST_D': {2: 6, 4: 15, 5: 3.75}   # TEST_D 클러스터별 Threshold\n",
    "}\n",
    "\n",
    "# 최종 제출 파일 생성\n",
    "final_submission = create_submission_with_threshold(errors_df, cluster_info, thresholds)\n",
    "final_submission.to_csv(output_path, index=False)\n",
    "print(f\"Results saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yangpa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
